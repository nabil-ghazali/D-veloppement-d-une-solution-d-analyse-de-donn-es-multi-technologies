{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psycopg2-binary in ./.venv/lib/python3.12/site-packages (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy in ./.venv/lib/python3.12/site-packages (2.0.42)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.1)\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.12/site-packages (from sqlalchemy) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in ./.venv/lib/python3.12/site-packages (from sqlalchemy) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary sqlalchemy pandas python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ytLvCF3fQxRJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connexion r√©ussie √† Supabase PostgreSQL.\n",
            "Version PostgreSQL : PostgreSQL 17.4 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 13.2.0, 64-bit\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration de connexion (√† adapter selon votre provider)\n",
        "DATABASE_CONFIG={\n",
        "    \"host\": \"aws-0-eu-west-3.pooler.supabase.com\", \n",
        "    \"database\": \"postgres\", \n",
        "    \"user\": \"postgres.xtfznwmlhngbzixfzwgq\", \n",
        "    \"password\": \"mentos\", \n",
        "    \"port\": 5432\n",
        "    }\n",
        "\n",
        "\n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@\"\n",
        "    f\"{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
        ")\n",
        "\n",
        "# Test de connexion\n",
        "def test_connection():\n",
        "    \"\"\"\n",
        "    Testez votre connexion √† la base\n",
        "\n",
        "    √âtapes :\n",
        "    1. Utilisez pd.read_sql() pour ex√©cuter \"SELECT version()\"\n",
        "    2. Affichez la version PostgreSQL\n",
        "    3. G√©rez les erreurs de connexion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "                # √âtape 1 & 2 : test via pd.read_sql()\n",
        "                version_df = pd.read_sql(\"SELECT version();\", conn)\n",
        "                print(\"Connexion r√©ussie √† Supabase PostgreSQL.\")\n",
        "                print(\"Version PostgreSQL :\", version_df.iloc[0, 0])\n",
        "                return True\n",
        "    except Exception as e:\n",
        "            print(f\"‚ùå Erreur de connexion : {e}\")\n",
        "            return False\n",
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partie 2-1 : Analyse des donn√©es import√©es dans le dossier data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        customer_id          customer_city customer_state\n",
            "0  06b8999e2fba1a1fbc88172c00ba8bc7                 franca             SP\n",
            "1  18955e83d337fd6b2def6b18a428ac77  sao bernardo do campo             SP\n",
            "2  4e7b3e00288586ebd08712fdd0374a03              sao paulo             SP\n",
            "3  b2b6027bc5c5109e529d4dc6358b12c3        mogi das cruzes             SP\n",
            "4  4f2d8ab171c80ec8364f7c12e35b23ad               campinas             SP\n",
            "customer_id       object\n",
            "customer_city     object\n",
            "customer_state    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Lire le fichier CSV des clients\n",
        "df_customers = pd.read_csv(\"data/olist_customers_dataset.csv\")\n",
        "\n",
        "# Garder uniquement les colonnes n√©cessaires\n",
        "df_customers = df_customers[[\"customer_id\", \"customer_city\", \"customer_state\"]]\n",
        "\n",
        "# Afficher les premi√®res lignes\n",
        "print(df_customers.head())\n",
        "print(df_customers.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           order_id                       customer_id  \\\n",
            "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
            "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
            "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
            "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
            "\n",
            "  order_status           order_date order_delivered_date  \n",
            "0    delivered  2017-10-02 10:56:33  2017-10-10 21:25:13  \n",
            "1    delivered  2018-07-24 20:41:37  2018-08-07 15:27:45  \n",
            "2    delivered  2018-08-08 08:38:49  2018-08-17 18:06:29  \n",
            "3    delivered  2017-11-18 19:28:06  2017-12-02 00:28:42  \n",
            "4    delivered  2018-02-13 21:18:39  2018-02-16 18:17:02  \n",
            "order_id                object\n",
            "customer_id             object\n",
            "order_status            object\n",
            "order_date              object\n",
            "order_delivered_date    object\n",
            "dtype: object\n",
            "                           order_id                       customer_id  \\\n",
            "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
            "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
            "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
            "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
            "\n",
            "  order_status           order_date order_delivered_date  \n",
            "0    delivered  2017-10-02 10:56:33  2017-10-10 21:25:13  \n",
            "1    delivered  2018-07-24 20:41:37  2018-08-07 15:27:45  \n",
            "2    delivered  2018-08-08 08:38:49  2018-08-17 18:06:29  \n",
            "3    delivered  2017-11-18 19:28:06  2017-12-02 00:28:42  \n",
            "4    delivered  2018-02-13 21:18:39  2018-02-16 18:17:02  \n",
            "order_id                object\n",
            "customer_id             object\n",
            "order_status            object\n",
            "order_date              object\n",
            "order_delivered_date    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Lire le fichier des commandes\n",
        "df_orders = pd.read_csv(\"data/olist_orders_dataset.csv\")\n",
        "\n",
        "# # Garder les colonnes utiles\n",
        "df_orders = df_orders[[\n",
        "    \"order_id\",\n",
        "    \"customer_id\",\n",
        "    \"order_status\",\n",
        "    \"order_purchase_timestamp\",\n",
        "    \"order_delivered_customer_date\"\n",
        "]]\n",
        "\n",
        "# Renommer les colonnes\n",
        "df_orders.rename(columns={\n",
        "    \"order_purchase_timestamp\": \"order_date\",\n",
        "    \"order_delivered_customer_date\": \"order_delivered_date\"\n",
        "}, inplace=True)\n",
        "\n",
        "# V√©rifier le r√©sultat\n",
        "print(df_orders.head())\n",
        "print(df_orders.dtypes)\n",
        "\n",
        "# Afficher les premi√®res lignes\n",
        "print(df_orders.head())\n",
        "print(df_orders.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           order_id                        product_id  \\\n",
            "0  00010242fe8c5a6d1ba2dd792cb16214  4244733e06e7ecb4970a6e2683c13e61   \n",
            "1  00018f77f2f0320c557190d7a144bdd3  e5f2d52b802189ee658865ca93d83a8f   \n",
            "2  000229ec398224ef6ca0657da4fc703e  c777355d18b72b67abbeef9df44fd0fd   \n",
            "3  00024acbcdf0a6daa1e931b038114c75  7634da152a4610f1595efa32f14722fc   \n",
            "4  00042b26cf59d7ce69dfabb4e55b4fd9  ac6c3623068f30de03045865e4e10089   \n",
            "\n",
            "                          seller_id   price  freight_value  \n",
            "0  48436dade18ac8b2bce089ec2a041202   58.90          13.29  \n",
            "1  dd7ddc04e1b6c2c614352b383efe2d36  239.90          19.93  \n",
            "2  5b51032eddd242adc84c38acab88f23d  199.00          17.87  \n",
            "3  9d7a1d34a5052409006425275ba1c2b4   12.99          12.79  \n",
            "4  df560393f3a51e74553ab94004ba5c87  199.90          18.14  \n",
            "order_id          object\n",
            "product_id        object\n",
            "seller_id         object\n",
            "price            float64\n",
            "freight_value    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Charger le fichier CSV des items de commande\n",
        "df_order_items = pd.read_csv(\"data/olist_order_items_dataset.csv\")\n",
        "\n",
        "# Garder uniquement les colonnes n√©cessaires\n",
        "df_order_items = df_order_items[[\n",
        "    \"order_id\",\n",
        "    \"product_id\",\n",
        "    \"seller_id\",\n",
        "    \"price\",\n",
        "    \"freight_value\"\n",
        "]]\n",
        "\n",
        "# Aper√ßu\n",
        "print(df_order_items.head())\n",
        "print(df_order_items.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         product_id       product_category  product_weight_g\n",
            "0  1e9e8ef04dbcff4541ed26657ea517e5             perfumaria             225.0\n",
            "1  3aa071139cb16b67ca9e5dea641aaa2f                  artes            1000.0\n",
            "2  96bd76ec8810374ed1b65e291975717f          esporte_lazer             154.0\n",
            "3  cef67bcfe19066a932b7673e239eb23d                  bebes             371.0\n",
            "4  9dc1a7de274444849c219cff195d0b71  utilidades_domesticas             625.0\n",
            "product_id           object\n",
            "product_category     object\n",
            "product_weight_g    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Charger le fichier des produits\n",
        "df_products = pd.read_csv(\"data/olist_products_dataset.csv\")\n",
        "\n",
        "# Garder les colonnes n√©cessaires\n",
        "df_products = df_products[[\n",
        "    \"product_id\",\n",
        "    \"product_category_name\",\n",
        "    \"product_weight_g\"\n",
        "]]\n",
        "\n",
        "# Renommer la colonne product_category_name en product_category\n",
        "df_products.rename(columns={\"product_category_name\": \"product_category\"}, inplace=True)\n",
        "\n",
        "# Afficher un aper√ßu\n",
        "print(df_products.head())\n",
        "print(df_products.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          seller_id        seller_city seller_state\n",
            "0  3442f8959a84dea7ee197c632cb2df15           campinas           SP\n",
            "1  d1b65fc7debc3361ea86b5f14c68d2e2         mogi guacu           SP\n",
            "2  ce3ad9de960102d0677a81f5d0bb7b2d     rio de janeiro           RJ\n",
            "3  c0f3eea2e14555b6faeea3dd58c1b1c3          sao paulo           SP\n",
            "4  51a04a8a6bdcb23deccc82b0b80742cf  braganca paulista           SP\n",
            "seller_id       object\n",
            "seller_city     object\n",
            "seller_state    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# 5. **sellers** : seller_id, seller_city, seller_state\n",
        "# Charger le fichier des vendeurs\n",
        "df_sellers = pd.read_csv(\"data/olist_sellers_dataset.csv\")\n",
        "\n",
        "# Garder les colonnes utiles\n",
        "df_sellers = df_sellers[[\n",
        "    \"seller_id\",\n",
        "    \"seller_city\",\n",
        "    \"seller_state\"\n",
        "]]\n",
        "\n",
        "# Afficher un aper√ßu\n",
        "print(df_sellers.head())\n",
        "print(df_sellers.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partie 2-2 : Cr√©ation des tables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GqGIXooNSTjp"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "def create_tables(engine):\n",
        "    \"\"\"\n",
        "    Cr√©e les tables dans PostgreSQL avec contraintes et index.\n",
        "    \"\"\"\n",
        "\n",
        "    create_queries = [\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS customers (\n",
        "            customer_id TEXT PRIMARY KEY,\n",
        "            customer_city TEXT,\n",
        "            customer_state TEXT\n",
        "        );\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS orders (\n",
        "            order_id TEXT PRIMARY KEY,\n",
        "            customer_id TEXT REFERENCES customers(customer_id),\n",
        "            order_status TEXT,\n",
        "            order_date TIMESTAMP,\n",
        "            order_delivered_date TIMESTAMP\n",
        "        );\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS products (\n",
        "            product_id TEXT PRIMARY KEY,\n",
        "            product_category TEXT,\n",
        "            product_weight_g FLOAT\n",
        "        );\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS sellers (\n",
        "            seller_id TEXT PRIMARY KEY,\n",
        "            seller_city TEXT,\n",
        "            seller_state TEXT\n",
        "        );\n",
        "        \"\"\",\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS order_items (\n",
        "            order_item_id SERIAL PRIMARY KEY,\n",
        "            order_id TEXT REFERENCES orders(order_id),\n",
        "            product_id TEXT REFERENCES products(product_id),\n",
        "            seller_id TEXT REFERENCES sellers(seller_id),\n",
        "            price FLOAT,\n",
        "            freight_value FLOAT\n",
        "        );\n",
        "        \"\"\",\n",
        "        # Index sur les cl√©s √©trang√®res (optionnel mais conseill√©)\n",
        "        \"CREATE INDEX IF NOT EXISTS idx_orders_customer_id ON orders(customer_id);\",\n",
        "        \"CREATE INDEX IF NOT EXISTS idx_order_items_order_id ON order_items(order_id);\",\n",
        "        \"CREATE INDEX IF NOT EXISTS idx_order_items_product_id ON order_items(product_id);\",\n",
        "        \"CREATE INDEX IF NOT EXISTS idx_order_items_seller_id ON order_items(seller_id);\"\n",
        "    ]\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        with conn.begin():\n",
        "            for query in create_queries:\n",
        "                conn.execute(text(query))\n",
        "        print(\"Toutes les tables ont √©t√© cr√©√©es.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toutes les tables ont √©t√© cr√©√©es.\n"
          ]
        }
      ],
      "source": [
        "# Supposons que tu as d√©j√† cr√©√© l'engine SQLAlchemy\n",
        "# from sqlalchemy import create_engine\n",
        "# engine = create_engine(\"postgresql://user:password@host:port/dbname\")\n",
        "\n",
        "create_tables(engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partie 2-1 : Analyse des donn√©es import√©es dans le dossier data  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "-- Cr√©ez les segments RFM (Champions, Loyaux, √Ä risque, etc.)\n",
        "SELECT\n",
        "    customer_id,\n",
        "    customer_state,\n",
        "    recency_score,\n",
        "    frequency_score,\n",
        "    monetary_score,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        -- Ajoutez les autres segments\n",
        "        ELSE 'Others'\n",
        "    END as customer_segment\n",
        "FROM customer_metrics;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
